{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KbB5c1x1dxi"
   },
   "source": [
    "# Tensorflow Keras Tutorial - Neural Network (Part 1)\n",
    "\n",
    "**What is Keras?** Keras is a user-friendly wrapper that simplifies the implementation of Deep Neural Networks without delving into intricate network details. It can utilize either *Tensorflow* or *Theano* as its backend. This tutorial series is designed to take you from a beginner to an intermediate level in understanding Keras.\n",
    "\n",
    "## In this part, we will cover:\n",
    "\n",
    "- Loading the MNIST Digit dataset\n",
    "- Basic preprocessing of image data\n",
    "- Training a simple neural network\n",
    "- Validating our trained model\n",
    "- Implementing early stopping when reaching desired accuracy or loss levels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.13.0\n",
      "  Downloading tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "     -------------------------------------- 276.5/276.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.6 MB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     ---------------------------------------- 24.4/24.4 MB 2.8 MB/s eta 0:00:00\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.57.0-cp310-cp310-win_amd64.whl (4.3 MB)\n",
      "     ---------------------------------------- 4.3/4.3 MB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (22.0)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 1.7 MB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "     -------------------------------------- 440.8/440.8 kB 3.4 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-4.24.0-cp310-abi3-win_amd64.whl (430 kB)\n",
      "     -------------------------------------- 430.5/430.5 kB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     -------------------------------------- 126.5/126.5 kB 3.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (65.6.3)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 3.0 MB/s eta 0:00:00\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.5/57.5 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.2)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.8/181.8 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.14)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.57.0 keras-2.13.1 libclang-16.0.6 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.24.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.13.0\n",
      "  Using cached tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.57.0)\n",
      "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.24.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\4dmun\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Installing collected packages: tensorflow-intel, tensorflow\n",
      "Successfully installed tensorflow-2.13.0 tensorflow-intel-2.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ENxm5MQ1dxl"
   },
   "source": [
    "# Step 1 - Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:44.34311Z",
     "start_time": "2020-06-10T15:43:39.819241Z"
    },
    "id": "UJiZ92Xz1dxm"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import necessary libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m  \u001b[38;5;66;03m# Library for creating visualizations\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m  \u001b[38;5;66;03m# Open-source machine learning framework\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m  \u001b[38;5;66;03m# Numerical computing library\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Display visualizations directly in the notebook\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import matplotlib.pyplot as plt  # Library for creating visualizations\n",
    "import tensorflow as tf  # Open-source machine learning framework\n",
    "import numpy as np  # Numerical computing library\n",
    "\n",
    "# Display visualizations directly in the notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-Lp4GpR1dxn"
   },
   "source": [
    "# Step 2 - Importing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmrbcyY031th"
   },
   "source": [
    "\n",
    "Here we are loading mnist Dataset which is preloaded in tensorflow. <br>\n",
    "\n",
    ">```mnist = tf.keras.datasets.mnist```<br>\n",
    "This returns the dataset object. Similarly there are 6 more datasets preloaded in keras.\n",
    "\n",
    ">Calling the `load_data` function on this object returns splitted train and test data in form of (features, target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:44.537002Z",
     "start_time": "2020-06-10T15:43:44.344753Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zkb4mDSu1dxn",
    "outputId": "fbbb2071-4115-4489-95f7-13536a15f32c"
   },
   "outputs": [],
   "source": [
    "# Load the MNIST dataset using TensorFlow's Keras API\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "# Load and unpack the training and testing data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YESEl9uH1dxo"
   },
   "source": [
    "## Overview of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLeGaPUm1dxo"
   },
   "source": [
    "## Dataset Overview\n",
    "\n",
    "The MNIST dataset consists of images, each having a resolution of 28x28 pixels. The dataset is divided into training and test sets, containing 60000 and 10000 images respectively.\n",
    "\n",
    "- The shape `(60000, 28, 28)` indicates that the training data contains 60000 images, each with dimensions 28x28.\n",
    "\n",
    "- The shape `(60000,)`, equivalent to `(60000, 1)`, represents the training labels. There are 60000 labels, one for each image.\n",
    "\n",
    "In summary, the dataset comprises images of handwritten digits, and each image is associated with a corresponding label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:44.542962Z",
     "start_time": "2020-06-10T15:43:44.539101Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Uof5JQ_1dxo",
    "outputId": "0cfae991-abcb-49b3-886b-ef96fcdf34c8"
   },
   "outputs": [],
   "source": [
    "# Print the shapes of the loaded data and labels\n",
    "print(f'Shape of the training data: {x_train.shape}')\n",
    "print(f'Shape of the training target: {y_train.shape}')\n",
    "print(f'Shape of the test data: {x_test.shape}')\n",
    "print(f'Shape of the test target: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:44.549424Z",
     "start_time": "2020-06-10T15:43:44.546448Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUV9ul121dxp",
    "outputId": "95c07f96-c7bd-4cc9-c843-3fff8bbfed05"
   },
   "outputs": [],
   "source": [
    "# Print the training labels\n",
    "print(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:44.77822Z",
     "start_time": "2020-06-10T15:43:44.551094Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "O4HugYeK1dxp",
    "outputId": "d6318d37-0d87-476d-b888-b7c6a5d21f3d"
   },
   "outputs": [],
   "source": [
    "# Let's plot the first image in the training data and look at its corresponding target (y) variable.\n",
    "\n",
    "# Display the first training image using a grayscale colormap\n",
    "plt.imshow(x_train[0], cmap='gray')\n",
    "\n",
    "# Print the corresponding target (label) for the first training image\n",
    "print(f'Target variable is {y_train[0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh3P9HCo1dxp"
   },
   "source": [
    "# Step 3 - Preprocess - Data Normalization\n",
    "\n",
    "The pixel values in the image data range from 0 to 255, representing grayscale intensity. To better suit the neural network's input requirements, we'll scale these values to a range of 0 to 1. This scaling process is known as **Normalization**.\n",
    "\n",
    "> Note: While normalization is beneficial for training neural networks, it's not mandatory. You can opt to skip these lines and observe the impact on the final output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:44.786318Z",
     "start_time": "2020-06-10T15:43:44.780474Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X955Xlxt1dxq",
    "outputId": "024425af-8b18-4b9d-a34b-7419dcc5aa6d"
   },
   "outputs": [],
   "source": [
    "# Setting custom printwidth to print the array properly\n",
    "np.set_printoptions(linewidth=200)\n",
    "\n",
    "# Print the pixel values of the first training image\n",
    "print(x_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:45.040786Z",
     "start_time": "2020-06-10T15:43:44.788783Z"
    },
    "id": "XTwxgvwG1dxq"
   },
   "outputs": [],
   "source": [
    "# Normalizing the data\n",
    "# Each element of the nested list/array is divided by 255 to normalize the pixel values between 0 and 1.\n",
    "# This normalization is commonly performed to improve the training stability of neural networks.\n",
    "x_train = x_train / 255\n",
    "x_test = x_test / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:45.054987Z",
     "start_time": "2020-06-10T15:43:45.044413Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G51t0gDf1dxq",
    "outputId": "12e1b4f1-c3b3-4223-ea07-30a20999f8f1"
   },
   "outputs": [],
   "source": [
    "# Print the normalized pixel values of the first training image after normalization\n",
    "print(x_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcvMLRTb1dxq"
   },
   "source": [
    "# Step 4 - Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sn5eAeQF4YO3"
   },
   "source": [
    "## Types of Models in TensorFlow\n",
    "\n",
    "There are two primary types of models in TensorFlow:\n",
    "\n",
    "1. **Sequential** - Discussed in this tutorial\n",
    "2. **Graphical**\n",
    "\n",
    "## Models\n",
    "\n",
    "- `tf.keras.models.Sequential()`: This function allows you to create a linear stack of layers, resulting in a sequential neural network.\n",
    "\n",
    "- `tf.model()`: This function enables you to construct an arbitrary graph of layers, as long as there are no cycles.\n",
    "\n",
    "## Flatten Layer\n",
    "\n",
    "- `tf.keras.layers.Flatten()`: This layer is used to flatten the input. For input of shape `(batch_size, height, width)`, the output is reshaped to `(batch_size, height * width)`.\n",
    "\n",
    "## Dense Layer\n",
    "\n",
    "- `tf.keras.layers.Dense()`: Represents a normal dense layer in a neural network, where each node is connected to every node in the next layer.\n",
    "\n",
    "    - **units**: Corresponds to the number of nodes in the layer.\n",
    "    - **activation**: An element-wise activation function.\n",
    "    \n",
    "        - **relu**: Converts negative values to 0 while keeping positive values unchanged.\n",
    "        - **softmax**: Converts the element with the maximum value to 1 and the rest to 0.\n",
    "\n",
    "In the example below, we have three dense layers with 128, 64, and 10 nodes respectively. The layer with 10 nodes is intended to be the output layer. We use the *softmax* activation function for the final/output layer to obtain a single value as 1 (preferably the maximum value).\n",
    "\n",
    "## Compiling the Model\n",
    "\n",
    "The `model.compile()` function configures the optimizer, loss, and metrics for training.\n",
    "\n",
    "- **optimizer**: Updates the parameters of the neural network.\n",
    "- **loss**: Measures the error in our model.\n",
    "- **metrics**: Used for evaluating the model's performance. While metrics are not used for training, loss evaluates the model's error during training and guides the optimizer in minimizing the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:45.112658Z",
     "start_time": "2020-06-10T15:43:45.057253Z"
    },
    "id": "EIe3pSi51dxr"
   },
   "outputs": [],
   "source": [
    "# Creating the architecture of the model\n",
    "# A Sequential model is created, and layers are added to define the neural network architecture.\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Flattening layer: Reshapes the input data into a 1D array before passing it to the neural network.\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Dense layers: Fully connected layers with specified number of units and activation functions.\n",
    "# Here, we add two hidden layers with 128 and 64 units respectively, using ReLU activation.\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(64, activation=tf.nn.relu))\n",
    "\n",
    "# Output layer: Final layer with 10 units (one for each digit) and softmax activation for classification.\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "# Compiling the model\n",
    "# The model is compiled with an optimizer, a loss function, and evaluation metrics.\n",
    "model.compile(\n",
    "    optimizer='adam',  # 'adam' optimizer for adaptive learning rates\n",
    "    loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification\n",
    "    metrics=['accuracy']  # Metric to monitor during training and evaluation\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrlhIZJa1dxr"
   },
   "source": [
    "# Step 5 - Training\n",
    "\n",
    "The `model.fit` Method for Training\n",
    "\n",
    "- **x_train**: Training data or features.\n",
    "- **y_train**: Target labels.\n",
    "- **epochs**: Number of times the entire dataset is fed into the model.\n",
    "\n",
    "During the training process, you can observe the loss and accuracy calculated based on the training data. However, determining the number of epochs is a trial-and-error process. It depends on various factors, such as the dataset size and the complexity of classification. With experience, you will develop an intuition for estimating the appropriate number of epochs required for a specific model and dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:50.170508Z",
     "start_time": "2020-06-10T15:43:45.114421Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cl-4Jq4n1dxr",
    "outputId": "235d6a2e-a5e7-4bf0-d653-728ccd49bb59"
   },
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# The model is trained using the training data (x_train) and labels (y_train) for a specified number of epochs.\n",
    "# An epoch represents one complete pass through the training dataset.\n",
    "model.fit(x_train, y_train, epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb_xl2N81dxr"
   },
   "source": [
    "# Step 6 - Validation\n",
    "\n",
    "## Assessing Model Accuracy on New Data\n",
    "\n",
    "We've validated our model's accuracy on the training data, achieving an approximate accuracy of 97%. Now, let's examine how well the model performs on new, unseen data. It's common for this validation accuracy to be slightly lower than the training accuracy.\n",
    "\n",
    "> Validation serves a critical purpose in preventing **overfitting**. Overfitting occurs when a model excels on training data but struggles on new test data. Large disparities between training and validation accuracies are indicative of overfitting.\n",
    "\n",
    "Detecting overfitting is crucial. When a notable gap exists between training and validation accuracy, it's a sign of potential overfitting. Fortunately, strategies to mitigate overfitting are available and will be explored later in this course.\n",
    "\n",
    "*Remember, while a modest decrease in validation accuracy is expected, a significant drop signals a need for further investigation.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:50.549877Z",
     "start_time": "2020-06-10T15:43:50.172155Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bTnBdltr1dxs",
    "outputId": "4f9a3c06-b7ca-45b1-fb6a-9a52d9b972df"
   },
   "outputs": [],
   "source": [
    "# Evaluating the trained model on the test data\n",
    "# The model's performance is evaluated using the test data (x_test) and test labels (y_test).\n",
    "\n",
    "# Evaluate the model and store the validation loss and accuracy\n",
    "val_loss, val_acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the validation loss and accuracy\n",
    "print(f'Validation loss: {val_loss}')\n",
    "print(f'Validation accuracy: {val_acc}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZLL_MC41dxs"
   },
   "source": [
    "# Step 7 - Stopping at Reaching Target Accuracy\n",
    "\n",
    "## Early Stopping for Desired Accuracy\n",
    "\n",
    "Consider a scenario where your goal is to achieve a model accuracy of 95%. However, you're uncertain about the optimal number of epochs required to attain this accuracy. You might set a high number of epochs, but if the model reaches the desired accuracy early on, continuing training could lead to overfitting. To address this, you can implement a mechanism to automatically stop training once the accuracy reaches 95%. Let's explore how to achieve this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:50.555674Z",
     "start_time": "2020-06-10T15:43:50.551212Z"
    },
    "id": "JU1C-h5D1dxs"
   },
   "outputs": [],
   "source": [
    "# Callback class which checks on the logs when the epoch ends\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # This method is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Check if the loss is below a certain threshold\n",
    "        if logs.get('loss') < 0.05:\n",
    "            print(\"\\nReached minimal loss, so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Create an instance of the callback class\n",
    "callbacks = myCallback()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T15:43:58.960603Z",
     "start_time": "2020-06-10T15:43:50.557116Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VzeuV_141dxt",
    "outputId": "492a00fd-fd36-4e03-c3da-1481b58881ce"
   },
   "outputs": [],
   "source": [
    "# Creating the architecture of the model\n",
    "# A Sequential model is created, and layers are added to define the neural network architecture.\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "# Flattening layer: Reshapes the input data into a 1D array before passing it to the neural network.\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "# Dense layers: Fully connected layers with specified number of units and activation functions.\n",
    "# Here, we add two hidden layers with 128 units each, using ReLU activation.\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(128, activation=tf.nn.relu))\n",
    "\n",
    "# Output layer: Final layer with 10 units (one for each digit) and softmax activation for classification.\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "# Compiling the model\n",
    "# The model is compiled with an optimizer, a loss function, and evaluation metrics.\n",
    "model.compile(\n",
    "    optimizer='adam',  # 'adam' optimizer for adaptive learning rates\n",
    "    loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification\n",
    "    metrics=['accuracy']  # Metric to monitor during training and evaluation\n",
    ")\n",
    "\n",
    "# Callback class which checks on the logs when the epoch ends\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    # This method is called at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        # Check if the loss is below a certain threshold\n",
    "        if logs.get('loss') < 0.05:\n",
    "            print(\"\\nReached minimal loss, so cancelling training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Create an instance of the callback class\n",
    "callbacks = myCallback()\n",
    "\n",
    "# Training the model with the custom callback applied\n",
    "model.fit(x_train, y_train, epochs=50, callbacks=[callbacks])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qWyxJ9sL1dxt"
   },
   "source": [
    "## Early Stopping for Target Loss\n",
    "\n",
    "It's important to note that even though we've set the number of epochs to 50, the training process can complete sooner. As soon as the loss drops below 0.05, the training halts. Alternatively, you can also monitor and use the ```accuracy``` parameter instead of ```loss``` for early stopping. Feel free to experiment with both approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kp_02uWY5ez_"
   },
   "source": [
    "\n",
    "**In the next tutorial, we will explore the advantages of a Simple Convolutional Neural Network (CNN) over the basic neural network we've discussed here. Stay tuned for deeper insights!**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5ENxm5MQ1dxl"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
